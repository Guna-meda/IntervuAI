import { useState, useEffect, useRef } from 'react';
import WebRTCService from '../../services/webrtcService';
import LiveWhisperService from '../../services/LiveWhisperService';

export default function LiveInterview({ 
  currentQuestion, 
  onUserResponse,
  onStatusUpdate 
}) {
  const [isListening, setIsListening] = useState(false);
  const [userTranscript, setUserTranscript] = useState('');
  const [videoStream, setVideoStream] = useState(null);
  const videoRef = useRef(null);

  // Initialize WebRTC stream when component mounts
  useEffect(() => {
    initializeWebRTC();
    
    return () => {
      // Cleanup
      WebRTCService.stopStream();
      LiveWhisperService.stopRecording(); // make sure recording is stopped
    };
  }, []);

  // Start listening automatically when a new question comes
  useEffect(() => {
    if (currentQuestion) {
      setTimeout(() => {
        startListening();
      }, 3000); // optional delay
    }
  }, [currentQuestion]);

  // Initialize WebRTC video/audio stream
  const initializeWebRTC = async () => {
    try {
      const stream = await WebRTCService.getLiveStream();
      setVideoStream(stream);
      if (videoRef.current) videoRef.current.srcObject = stream;
    } catch (error) {
      console.error('Failed to initialize WebRTC:', error);
    }
  };

  // Start recording audio
  const startListening = async () => {
    setIsListening(true);
    setUserTranscript("üé§ Listening... Speak your answer");
    if (onStatusUpdate) onStatusUpdate('listening');

    try {
      await LiveWhisperService.startRecording();
    } catch (error) {
      console.error('Failed to start recording:', error);
      setUserTranscript('‚ùå Microphone access required');
      setIsListening(false);
    }
  };

  // Stop recording and send audio to backend
  const stopListening = async () => {
    try {
      const transcript = await LiveWhisperService.stopRecording();
      setIsListening(false);

      if (transcript) {
        setUserTranscript(transcript);
        if (onUserResponse) onUserResponse(transcript);
      } else {
        setUserTranscript("‚ùå Failed to transcribe audio");
      }

      if (onStatusUpdate) onStatusUpdate('processing');
    } catch (error) {
      console.error('Error stopping recording:', error);
      setUserTranscript("‚ùå Error processing audio");
      setIsListening(false);
    }
  };

  return (
    <div className="live-interview">
      <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
        {/* Video Section */}
        <div className="video-section">
          <h3 className="text-lg font-semibold mb-4">üìπ Live Video</h3>
          <div className="video-container bg-black rounded-lg overflow-hidden">
            <video
              ref={videoRef}
              autoPlay
              muted
              playsInline
              className="w-full h-64 lg:h-96 object-cover"
            />
          </div>
          <div className="mt-2 text-sm text-gray-600">
            {isListening ? '‚óè LIVE - Listening to your audio' : 'Video preview'}
          </div>
        </div>

        {/* Transcription Section */}
        <div className="transcription-section">
          <h3 className="text-lg font-semibold mb-4">üí¨ Live Transcription</h3>
          
          <div className={`p-4 rounded-lg min-h-48 ${
            isListening ? 'bg-green-50 border border-green-200' : 'bg-gray-50'
          }`}>
            {userTranscript ? (
              <p className="text-gray-800 whitespace-pre-wrap">{userTranscript}</p>
            ) : (
              <p className="text-gray-400 italic">
                {currentQuestion ? 'Waiting for you to speak...' : 'Waiting for question...'}
              </p>
            )}
          </div>

          {/* Status Indicator */}
          <div className="mt-4 p-3 rounded-lg bg-blue-50 border border-blue-200">
            <div className="flex items-center">
              <div className={`w-3 h-3 rounded-full mr-2 ${
                isListening ? 'bg-green-500 animate-pulse' : 'bg-gray-400'
              }`}></div>
              <span className="text-sm font-medium">
                {isListening ? 'Live Transcription Active' : 'Ready for Question'}
              </span>
            </div>
            <p className="text-xs text-gray-600 mt-1">
              {isListening 
                ? "Speak your answer. We'll send it when you click Stop." 
                : 'WebRTC stream active - video and audio ready'}
            </p>
          </div>

          {/* Controls */}
          <div className="mt-4 flex space-x-4">
            <button
              onClick={startListening}
              disabled={isListening || !currentQuestion}
              className="px-4 py-2 bg-green-500 text-white rounded disabled:bg-gray-400"
            >
              Start Speaking
            </button>
            <button
              onClick={stopListening}
              disabled={!isListening}
              className="px-4 py-2 bg-red-500 text-white rounded disabled:bg-gray-400"
            >
              Stop
            </button>
          </div>
        </div>
      </div>

      {/* Current Question Display */}
      {currentQuestion && (
        <div className="mt-6 p-4 bg-yellow-50 rounded-lg border border-yellow-200">
          <h3 className="font-semibold text-yellow-800 mb-2">ü§î Current Question:</h3>
          <p className="text-lg text-gray-800">{currentQuestion}</p>
        </div>
      )}
    </div>
  );
}
